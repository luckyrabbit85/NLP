{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spacy Features.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwbb5IvNfqi7"
      },
      "source": [
        "### Linguistic Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sutS0dAj79S6",
        "outputId": "c582005f-38f1-4378-9aba-0706f05ad70b"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 35.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xY1K-Kv9J8B"
      },
      "source": [
        "### POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQfxZmSi7smo",
        "outputId": "95c901e7-5fa9-4f7c-bef5-e8982042f0a6"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "doc = nlp(\"Alicia and me went to the school by bus\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_, spacy.explain(token.pos_), spacy.explain(token.tag_))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alicia PROPN NNP proper noun noun, proper singular\n",
            "and CCONJ CC coordinating conjunction conjunction, coordinating\n",
            "me PRON PRP pronoun pronoun, personal\n",
            "went VERB VBD verb verb, past tense\n",
            "to ADP IN adposition conjunction, subordinating or preposition\n",
            "the DET DT determiner determiner\n",
            "school NOUN NN noun noun, singular or mass\n",
            "by ADP IN adposition conjunction, subordinating or preposition\n",
            "bus NOUN NN noun noun, singular or mass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcgzhVGs8gnP",
        "outputId": "8e39afde-7ddf-413b-c4a4-b826a638c9c2"
      },
      "source": [
        "doc = nlp(\"My friend will fly to New York fast and she is staying there for 3 days.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_, spacy.explain(token.pos_), spacy.explain(token.tag_))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My DET PRP$ determiner pronoun, possessive\n",
            "friend NOUN NN noun noun, singular or mass\n",
            "will VERB MD verb verb, modal auxiliary\n",
            "fly VERB VB verb verb, base form\n",
            "to ADP IN adposition conjunction, subordinating or preposition\n",
            "New PROPN NNP proper noun noun, proper singular\n",
            "York PROPN NNP proper noun noun, proper singular\n",
            "fast ADV RB adverb adverb\n",
            "and CCONJ CC coordinating conjunction conjunction, coordinating\n",
            "she PRON PRP pronoun pronoun, personal\n",
            "is AUX VBZ auxiliary verb, 3rd person singular present\n",
            "staying VERB VBG verb verb, gerund or present participle\n",
            "there ADV RB adverb adverb\n",
            "for ADP IN adposition conjunction, subordinating or preposition\n",
            "3 NUM CD numeral cardinal number\n",
            "days NOUN NNS noun noun, plural\n",
            ". PUNCT . punctuation punctuation mark, sentence closer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDv-ino7BIyY",
        "outputId": "26dd6f70-34f9-450b-b960-cc8d957a0791"
      },
      "source": [
        "doc = nlp(\"My cat will fish for a fish tomorrow in a fishy way.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_, spacy.explain(token.pos_), spacy.explain(token.tag_))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My DET PRP$ determiner pronoun, possessive\n",
            "cat NOUN NN noun noun, singular or mass\n",
            "will VERB MD verb verb, modal auxiliary\n",
            "fish VERB VB verb verb, base form\n",
            "for ADP IN adposition conjunction, subordinating or preposition\n",
            "a DET DT determiner determiner\n",
            "fish NOUN NN noun noun, singular or mass\n",
            "tomorrow NOUN NN noun noun, singular or mass\n",
            "in ADP IN adposition conjunction, subordinating or preposition\n",
            "a DET DT determiner determiner\n",
            "fishy ADJ JJ adjective adjective\n",
            "way NOUN NN noun noun, singular or mass\n",
            ". PUNCT . punctuation punctuation mark, sentence closer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTRX6GxVBpJ4",
        "outputId": "f08102c5-b8ec-47c9-e8f4-7759419b0f47"
      },
      "source": [
        "doc = nlp(\"He earned $5.5 million in 2020 and paid %35 tax.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_, spacy.explain(token.pos_), spacy.explain(token.tag_))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He PRON PRP pronoun pronoun, personal\n",
            "earned VERB VBD verb verb, past tense\n",
            "$ SYM $ symbol symbol, currency\n",
            "5.5 NUM CD numeral cardinal number\n",
            "million NUM CD numeral cardinal number\n",
            "in ADP IN adposition conjunction, subordinating or preposition\n",
            "2020 NUM CD numeral cardinal number\n",
            "and CCONJ CC coordinating conjunction conjunction, coordinating\n",
            "paid VERB VBD verb verb, past tense\n",
            "% NOUN NN noun noun, singular or mass\n",
            "35 NUM CD numeral cardinal number\n",
            "tax NOUN NN noun noun, singular or mass\n",
            ". PUNCT . punctuation punctuation mark, sentence closer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZe8X3joIlqH"
      },
      "source": [
        "### Dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UNaLH9QBwpQ",
        "outputId": "51505a40-5097-4e3a-958c-afcbac91dd70"
      },
      "source": [
        "doc = nlp(\"I counted white sheep.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_, token.dep_, token.head)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON PRP nsubj counted\n",
            "counted VERB VBD ROOT counted\n",
            "white ADJ JJ amod sheep\n",
            "sheep PROPN NNP dobj counted\n",
            ". PUNCT . punct counted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "Fao21VZuD6N_",
        "outputId": "5fb24417-3272-408f-84e8-f31f2089e65e"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, jupyter=True, style='dep')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"cbb4af43aec445b093e431817dcc3776-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">counted</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">white</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">sheep.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cbb4af43aec445b093e431817dcc3776-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cbb4af43aec445b093e431817dcc3776-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cbb4af43aec445b093e431817dcc3776-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cbb4af43aec445b093e431817dcc3776-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-cbb4af43aec445b093e431817dcc3776-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-cbb4af43aec445b093e431817dcc3776-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4ouMGaYMFNI"
      },
      "source": [
        "### NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvlUQk3DHE_I",
        "outputId": "11574d07-c342-42bf-d6a3-382c28669f19"
      },
      "source": [
        "doc = nlp(\"The president Donald Trump visited France.\")\n",
        "print(doc.ents)\n",
        "print(type(doc.ents[1]))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Donald Trump, France)\n",
            "<class 'spacy.tokens.span.Span'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHLqgu5oHqY4",
        "outputId": "d1bc066e-be16-43c3-da25-015a1eef7b44"
      },
      "source": [
        "print(spacy.explain(\"ORG\"))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Companies, agencies, institutions, etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdkF5cHPHweQ",
        "outputId": "ecb776c5-6b72-4d51-e490-8ac2691b7762"
      },
      "source": [
        "doc2 = nlp(\"He worked for NASA\")\n",
        "token = doc2[3]\n",
        "print(token.text, token.ent_type_, spacy.explain(token.ent_type_))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NASA ORG Companies, agencies, institutions, etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTTeY5jyH1Fn",
        "outputId": "84886987-f84b-4e02-b1d4-f5b0f6514830"
      },
      "source": [
        "doc3 = nlp(\"“Albert Einstein was born in Ulm on 1987. He studied electronical engineering at ETH Zurich.\")\n",
        "print(doc3.ents)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Albert Einstein, Ulm, 1987, ETH Zurich)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvVWJbU4ICf_",
        "outputId": "3ab0c337-85f7-417f-bc35-d5530f9a0705"
      },
      "source": [
        "for token in doc3:\n",
        "    print(token.text, token.ent_type_, spacy.explain(token.ent_type_))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "“  None\n",
            "Albert PERSON People, including fictional\n",
            "Einstein PERSON People, including fictional\n",
            "was  None\n",
            "born  None\n",
            "in  None\n",
            "Ulm GPE Countries, cities, states\n",
            "on  None\n",
            "1987 DATE Absolute or relative dates or periods\n",
            ".  None\n",
            "He  None\n",
            "studied  None\n",
            "electronical  None\n",
            "engineering  None\n",
            "at  None\n",
            "ETH FAC Buildings, airports, highways, bridges, etc.\n",
            "Zurich FAC Buildings, airports, highways, bridges, etc.\n",
            ".  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLT3J4BxPeUP"
      },
      "source": [
        "### Merging-Splitting-Merge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glXWsaHPIPGn",
        "outputId": "a9b7a215-c4a6-403b-d7bb-410ba93d2281"
      },
      "source": [
        "doc = nlp(\"She lived in New Hampshire.\")\n",
        "print(doc.ents)\n",
        "print([(token.text, token.i) for token in doc])\n",
        "print(len(doc))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(New Hampshire,)\n",
            "[('She', 0), ('lived', 1), ('in', 2), ('New', 3), ('Hampshire', 4), ('.', 5)]\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bod5QVkwOmz2"
      },
      "source": [
        "with doc.retokenize() as retokenizer:\n",
        "    retokenizer.merge(doc[3:5], attrs={\"LEMMA\":\"new hampshire\"})"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S974ZYSkPGjv",
        "outputId": "30d53ade-4beb-4b59-b115-97df97e69912"
      },
      "source": [
        "print(doc.ents)\n",
        "print([(token.text, token.i) for token in doc])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(New Hampshire,)\n",
            "[('She', 0), ('lived', 1), ('in', 2), ('New Hampshire', 3), ('.', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0L2tZaNPK8X",
        "outputId": "57ff3c70-98f9-4b4a-96e1-8bc947f27a28"
      },
      "source": [
        "print(len(doc))\n",
        "print([(token.lemma_) for token in doc])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "['-PRON-', 'live', 'in', 'new hampshire', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLptmTy-PXnH",
        "outputId": "50019600-3e31-4fab-d0b3-ebba82fe2f48"
      },
      "source": [
        "doc = nlp(\"She lived in NewHampshire.\")\n",
        "print(len(doc))\n",
        "print([(token.text, token.lemma_, token.i) for token in doc])\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_, token.dep_)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "[('She', '-PRON-', 0), ('lived', 'live', 1), ('in', 'in', 2), ('NewHampshire', 'NewHampshire', 3), ('.', '.', 4)]\n",
            "She PRON PRP nsubj\n",
            "lived VERB VBD ROOT\n",
            "in ADP IN prep\n",
            "NewHampshire PROPN NNP pobj\n",
            ". PUNCT . punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxfxUUdpQkDO"
      },
      "source": [
        "with doc.retokenize() as retokenizer:\n",
        "    heads = [(doc[3], 1), doc[2]]\n",
        "    attrs = {\"TAG\":[\"NNP\", \"NNP\"], \"DEP\":[\"compound\", \"pobj\"]}\n",
        "    retokenizer.split(doc[3], [\"New\", \"Hampshire\"], heads=heads, attrs=attrs)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3y6-Fx-Qt7X",
        "outputId": "2450b85c-9c38-49f8-cfd4-40eadea5945b"
      },
      "source": [
        "print(len(doc))\n",
        "print([(token.text, token.lemma_, token.i) for token in doc])\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_, token.dep_)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "[('She', '-PRON-', 0), ('lived', 'live', 1), ('in', 'in', 2), ('New', 'New', 3), ('Hampshire', 'Hampshire', 4), ('.', '.', 5)]\n",
            "She PRON PRP nsubj\n",
            "lived VERB VBD ROOT\n",
            "in ADP IN prep\n",
            "New PROPN NNP compound\n",
            "Hampshire PROPN NNP pobj\n",
            ". PUNCT . punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avZcuWNifUVL"
      },
      "source": [
        "### Rule-Based Matching - Matcher Class\n",
        "\n",
        "Matching a pattern"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHhKl-x8QxOe"
      },
      "source": [
        "from spacy.matcher import Matcher"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ppvYMEjBXa",
        "outputId": "d5d0db62-bec6-4eda-f2fd-352052077a29"
      },
      "source": [
        "doc = nlp(\"Good morning, I want to reserve a ticket.\") \n",
        "\n",
        "matcher = Matcher(nlp.vocab) # Matcher needs to be intialized with vocabulary object\n",
        "pattern = [{\"LOWER\": \"good\"}, {\"LOWER\": \"morning\"}, {\"IS_PUNCT\": True}] \n",
        "matcher.add(\"morningGreeting\", [pattern]) \n",
        "\n",
        "matches = matcher(doc) \n",
        "for match_id, start, end in matches: \n",
        "    m_span = doc[start:end]   \n",
        "    print(start, end, m_span.text)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 3 Good morning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL6kZu_M5Rie"
      },
      "source": [
        "Matching two patterns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjE4HC4slcIz",
        "outputId": "8f7fc2bb-76e8-41f1-cfd0-b5d04a527c6c"
      },
      "source": [
        "doc = nlp(\"Good morning, I want to reserve a ticket. I will then say good evening!\") \n",
        "# Initialize the Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define the patterns\n",
        "pattern1 = [{\"LOWER\": \"good\"}, {\"LOWER\": \"morning\"}, {\"IS_PUNCT\": True}]\n",
        "pattern2 = [{\"LOWER\": \"good\"}, {\"LOWER\": \"evening\"}, {\"IS_PUNCT\": True}]  \n",
        "\n",
        "# name and add the patterns\n",
        "matcher.add(\"morningGreeting\", [pattern1]) \n",
        "matcher.add(\"eveningGreeting\", [pattern2]) \n",
        "\n",
        "# get the matches\n",
        "matches = matcher(doc) \n",
        "\n",
        "for match_id, start, end in matches: \n",
        "    pattern_name = nlp.vocab.strings[match_id]\n",
        "    m_span = doc[start:end]   \n",
        "    print(start, end, m_span.text) "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 3 Good morning,\n",
            "14 17 good evening!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LAoqbw-9pX9"
      },
      "source": [
        "While matching pattern ORTH and TEXT are similar to LOWER: they mean an exact match of the token text, including the case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFXfEoJn2bH2",
        "outputId": "1986da3b-04ad-4905-9a9c-9dac257193d5"
      },
      "source": [
        "doc = nlp(\"I bought a pineapple.\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"LENGTH\": 1}]\n",
        "\n",
        "matcher.add(\"onlyShort\",  [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "\n",
        "print(\"no.of matches:\", len(matches))\n",
        "\n",
        "for mid, start, end in matches:\n",
        "     print(start, end, doc[start:end])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no.of matches: 3\n",
            "0 1 I\n",
            "2 3 a\n",
            "4 5 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaHpPu52-PXc"
      },
      "source": [
        "The next block of token attributes is IS_ALPHA, IS_ASCII, and IS_DIGIT. These features are handy for finding number tokens and ordinary words (which do not include any interesting characters). The following pattern matches a sequence of two tokens, a number followed by an ordinary word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2380v65K9_vr",
        "outputId": "1f760ed3-575c-48af-924c-bcec11d09afa"
      },
      "source": [
        "doc1 = nlp(\"I met him at 2 o'clock.\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"IS_DIGIT\": True},{\"IS_ALPHA\": True}]\n",
        "\n",
        "matcher.add(\"numberAndPlainWord\",  [pattern])\n",
        "\n",
        "matches = matcher(doc1)\n",
        "\n",
        "print(len(matches))\n",
        "\n",
        "for mid, start, end in matches:\n",
        "     print(start, end, doc1[start:end])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmLioJoC-yc1",
        "outputId": "e9eecfa7-577e-43c0-f476-8e1c974fd65d"
      },
      "source": [
        "doc2 = nlp(\"He brought me 2 apples.\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"IS_DIGIT\": True},{\"IS_ALPHA\": True}]\n",
        "\n",
        "matcher.add(\"numberAndPlainWord\",  [pattern])\n",
        "\n",
        "matches = matcher(doc2)\n",
        "\n",
        "print(len(matches))\n",
        "\n",
        "for mid, start, end in matches:\n",
        "     print(start, end, doc2[start:end])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "3 5 2 apples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs257RcVOH7M"
      },
      "source": [
        "In the preceding code segment, 2 o'clock didn't match the pattern because o'clock contains an apostrophe, which is not an alphabetic character (alphabetic characters are digits, letters, and the underscore character). 2 apples matched because the token apples consists of letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaM7q81n_O3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59308e17-8c34-495e-e185-6ae2b66af74a"
      },
      "source": [
        "doc = nlp(\"Take me out of your SPAM list. We never asked you to contact me. If you write again we'll SUE!!!!\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"IS_UPPER\": True}]\n",
        "\n",
        "matcher.add(\"capitals\",  [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "\n",
        "for mid, start, end in matches:\n",
        "     print(start, end, doc[start:end])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 6 SPAM\n",
            "22 23 SUE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrxVBGibQ4Jy"
      },
      "source": [
        "doc1 = nlp(\"Can you swim?\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# In here we put two attributes in one  brace\n",
        "pattern = [{\"IS_SENT_START\": True, \"LOWER\": \"can\"}, {\"IS_TITLE\": True}]\n",
        "\n",
        "matcher.add(\"canThenCapitalized\",  [pattern])\n",
        "\n",
        "matches = matcher(doc1)\n",
        "\n",
        "for mid, start, end in matches:\n",
        "     print(start, end, doc1[start:end])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWMq_HKoRBZ7",
        "outputId": "350bf492-b9c5-4b4a-f29c-193bf9e8285f"
      },
      "source": [
        "doc2 = nlp(\"Can Sally swim?\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"IS_SENT_START\": True, \"LOWER\": \"can\"}, {\"IS_TITLE\": True}]\n",
        "\n",
        "matcher.add(\"canThenCapitalized\",  [pattern])\n",
        "\n",
        "matches = matcher(doc2)\n",
        "\n",
        "for mid, start, end in matches:\n",
        "     print(start, end, doc2[start:end])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2 Can Sally\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUj7P6RHSkok"
      },
      "source": [
        "LIKE_NUM, LIKE_URL, and LIKE_EMAIL are attributes that are related to token shape.\n",
        "\n",
        "After seeing the shape attributes, let's see the POS, TAG, DEP, LEMMA, and SHAPE linguistic attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2Rz5BiPRZf7",
        "outputId": "351b3f0c-fb9a-42de-deef-d5ed2772a107"
      },
      "source": [
        "doc = nlp(\"Will you go there?\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"IS_SENT_START\": True, \"TAG\": \"MD\"}]\n",
        "\n",
        "matcher.add(\"sentStart\",[pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "\n",
        "\n",
        "for mid, start, end in matches:\n",
        "     print(start, end, doc[start:end])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1 Will\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI-2phepVcld"
      },
      "source": [
        "Extended syntax support - IN, NOT_IN, IS_SUBSET, IS_SUPERSET, INTESECTS and comparison operators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUbhjgX0TNvL",
        "outputId": "4706bd0f-2c48-46c9-d07f-27ee0b4052b7"
      },
      "source": [
        "doc = nlp(\"Good morning, I'm here. I'll say good evening!!\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"LOWER\": \"good\"},{\"LOWER\": {\"IN\": [\"morning\", \"evening\"]}},{\"IS_PUNCT\": True}]\n",
        "\n",
        "matcher.add(\"greetings\",  [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "\n",
        "for mid, start, end in matches:\n",
        "     print(start, end, doc[start:end])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 3 Good morning,\n",
            "10 13 good evening!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7EtCuPGVhs7",
        "outputId": "8d62d324-3cec-4ab6-e980-f062f0aaf088"
      },
      "source": [
        "doc = nlp(\"I suffered from Trichotillomania when I was in college. The doctor prescribed me Psychosomatic medicine.\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"LENGTH\": {\">=\" : 10}}]\n",
        "\n",
        "matcher.add(\"longWords\",  [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "\n",
        "for mid, start, end in matches:\n",
        "     print(start, end, doc[start:end])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 4 Trichotillomania\n",
            "12 13 prescribed\n",
            "14 15 Psychosomatic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDbbEWskWLLy"
      },
      "source": [
        "Regex-like operators - OP\n",
        "```\n",
        "    !\tNegate the pattern, by requiring it to match exactly 0 times.\n",
        "    ?\tMake the pattern optional, by allowing it to match 0 or 1 times.\n",
        "    +\tRequire the pattern to match 1 or more times.\n",
        "    *\tAllow the pattern to match 0 or more times.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR6d1Lu6WOKC",
        "outputId": "e2363b31-6a32-47de-92d5-2a31072ef19a"
      },
      "source": [
        "doc1 = nlp(\"Barack Obama visited France.\")\n",
        "doc2 = nlp(\"Barack Hussein Obama visited France.\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"LOWER\": \"barack\"}, {\"LOWER\": \"hussein\", \"OP\": \"?\"},{\"LOWER\": \"obama\"}]\n",
        "\n",
        "matcher.add(\"obamaNames\",  [pattern])\n",
        "\n",
        "print(matcher(doc1))\n",
        "print(matcher(doc2))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(9957319642918298529, 0, 2)]\n",
            "[(9957319642918298529, 0, 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5TZVOkwXt2R",
        "outputId": "d0177629-8d7a-49be-c9b0-36a4787de462"
      },
      "source": [
        "doc1 = nlp(\"Hello hello hello, how are you?\")\n",
        "doc2 = nlp(\"Hello, how are you?\")\n",
        "doc3 = nlp(\"How are you?\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"LOWER\": {\"IN\": [\"hello\", \"hi\", \"hallo\"]}, \"OP\": \"*\"}, {\"IS_PUNCT\": True}]\n",
        "\n",
        "matcher.add(\"greetings\",  [pattern])\n",
        "\n",
        "print(\"**************\")\n",
        "\n",
        "for mid, start, end in matcher(doc1):\n",
        "     print(start, end, doc1[start:end])\n",
        "\n",
        "print(\"**************\")\n",
        "\n",
        "for mid, start, end in matcher(doc2):\n",
        "     print(start, end, doc1[start:end])\n",
        "\n",
        "print(\"**************\")\n",
        "\n",
        "for mid, start, end in matcher(doc3):\n",
        "     print(start, end, doc1[start:end])\n",
        "\n",
        "print(\"**************\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************\n",
            "0 4 Hello hello hello,\n",
            "1 4 hello hello,\n",
            "2 4 hello,\n",
            "3 4 ,\n",
            "7 8 ?\n",
            "**************\n",
            "0 2 Hello hello\n",
            "1 2 hello\n",
            "5 6 are\n",
            "**************\n",
            "3 4 ,\n",
            "**************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11u5Q6QabgQ0",
        "outputId": "ef7f7a38-0a84-47ad-cfb3-f71069328226"
      },
      "source": [
        "doc1 = nlp(\"Hello hello hello, how are you?\")\n",
        "doc2 = nlp(\"Hello, how are you?\")\n",
        "doc3 = nlp(\"How are you?\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"LOWER\": {\"IN\": [\"hello\", \"hi\", \"hallo\"]}, \"OP\": \"+\"}, {\"IS_PUNCT\": True}]\n",
        "\n",
        "matcher.add(\"greetings\", [pattern])\n",
        "\n",
        "print(\"**************\")\n",
        "\n",
        "for mid, start, end in matcher(doc1):\n",
        "     print(start, end, doc1[start:end])\n",
        "\n",
        "print(\"**************\")\n",
        "\n",
        "for mid, start, end in matcher(doc2):\n",
        "     print(start, end, doc1[start:end])\n",
        "\n",
        "print(\"**************\")\n",
        "\n",
        "for mid, start, end in matcher(doc3):\n",
        "     print(start, end, doc1[start:end])\n",
        "\n",
        "print(\"**************\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************\n",
            "2 4 hello,\n",
            "1 4 hello hello,\n",
            "0 4 Hello hello hello,\n",
            "**************\n",
            "0 2 Hello hello\n",
            "**************\n",
            "**************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI-1fH06YjEa"
      },
      "source": [
        "Regex support - spaCy Matcher offers full support for token-level regex matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhzDrjbEYfhz",
        "outputId": "ba6ad812-1705-4893-f406-3dfa3a4dcb0f"
      },
      "source": [
        "doc1 = nlp(\"I travelled by bus.\") \n",
        "doc2 = nlp(\"She traveled by bike.\") \n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"POS\": \"PRON\"}, {\"TEXT\": {\"REGEX\": \"[Tt]ravell?ed\"}}] \n",
        "\n",
        "matcher.add(\"travelRegex\", [pattern]) \n",
        "\n",
        "for mid, start, end in matcher(doc1): \n",
        "    print(start, end, doc1[start:end]) \n",
        "\n",
        "for mid, start, end in matcher(doc2): \n",
        "    print(start, end, doc2[start:end]) "
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2 I travelled\n",
            "0 2 She traveled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uICLAFmffhJ",
        "outputId": "773d6fda-a0f5-49b5-cc68-ca8332f8504c"
      },
      "source": [
        "doc = nlp(\"I went to Italy; he has been there too. His mother also has told me she wants to visit Rome.\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Using regex with POS tags\n",
        "pattern = [{\"TAG\": {\"REGEX\": \"^V\"}}]\n",
        "\n",
        "matcher.add(\"verbs\",  [pattern])\n",
        "\n",
        "for mid, start, end in matcher(doc):\n",
        "    print(start, end, doc1[start:end])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2 travelled\n",
            "6 7 \n",
            "7 8 \n",
            "14 15 \n",
            "15 16 \n",
            "18 19 \n",
            "20 21 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNE2o8zQcVzs"
      },
      "source": [
        "We have extracted all the finite verbs (you can think of a finite verb as a non-modal verb). How did we do it? Our token pattern includes the regex ^V, which means all fine-grained POS tags that start with V: VB, VGD, VBG, VBN, VBP, and VBZ. Then we extracted tokens with verbal POS tags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_mmJ2ALgFWi"
      },
      "source": [
        "Wild Card Matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZFTHiXCYgg6",
        "outputId": "d9bb3d66-2e7a-4be1-c211-a34abfbb396d"
      },
      "source": [
        "doc = nlp(\"My name is Alice and his name was Elliot.\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"LOWER\": \"name\"},{\"LEMMA\": \"be\"},{}]\n",
        "\n",
        "matcher.add(\"pickName\", [pattern])\n",
        "\n",
        "for mid, start, end in matcher(doc):\n",
        "     print(start, end, doc[start:end])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 4 name is Alice\n",
            "6 9 name was Elliot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoUbQfM9ciYq",
        "outputId": "1da4dfb2-417a-428e-de92-74ae44472a9c"
      },
      "source": [
        "doc1 = nlp(\"I forwarded his email to you.\")\n",
        "doc2 = nlp(\"I forwarded an email to you.\")\n",
        "doc3 = nlp(\"I forwarded the email to you.\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"LEMMA\": \"forward\"}, {}, {\"LOWER\": \"email\"}]\n",
        "\n",
        "matcher.add(\"forwardMail\",  [pattern])\n",
        "\n",
        "print(\"****************************\")\n",
        "\n",
        "for mid, start, end in matcher(doc1):\n",
        "     print(start, end, doc1[start:end])\n",
        "\n",
        "print(\"****************************\")\n",
        "\n",
        "for mid, start, end in matcher(doc2):\n",
        "     print(start, end, doc2[start:end])\n",
        "\n",
        "print(\"****************************\")\n",
        "\n",
        "for mid, start, end in matcher(doc3):\n",
        "    print(start, end, doc3[start:end])\n",
        "\n",
        "print(\"****************************\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************\n",
            "1 4 forwarded his email\n",
            "****************************\n",
            "1 4 forwarded an email\n",
            "****************************\n",
            "1 4 forwarded the email\n",
            "****************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDsNdDlt4fiT"
      },
      "source": [
        "To check regex and Matcher these sites are useful:\n",
        "\n",
        "https://regex101.com/  \n",
        "\n",
        "https://explosion.ai/demos/matcher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9qqYwPv4y5A"
      },
      "source": [
        "Phrase Matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB6EyoJOc8qp"
      },
      "source": [
        "from spacy.matcher import PhraseMatcher"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWOlfDpd43l6",
        "outputId": "530519bd-e685-4d47-fc62-b305f56702e5"
      },
      "source": [
        "doc = nlp(\"3 EU leaders met in Berlin. German chancellor Angela Merkel first welcomed the US president Donald Trump. The following day Alexis Tsipras joined them in Brandenburg.\")\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab) \n",
        "\n",
        "terms = [\"Angela Merkel\", \"Donald Trump\", \"Alexis Tsipras\"] \n",
        "patterns = [nlp.make_doc(term) for term in terms] \n",
        "# make_doc() creates a Doc from every term, and it's quite efficient in terms\n",
        "# of processing because instead of the whole pipeline, it only calls the Tokenizer\n",
        "matcher.add(\"politiciansList\", None, *patterns) \n",
        "\n",
        "matches = matcher(doc) \n",
        "\n",
        "for mid, start, end in matches: \n",
        "    print(start, end, doc[start:end]) "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9 11 Angela Merkel\n",
            "16 18 Donald Trump\n",
            "22 24 Alexis Tsipras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u43ksZ7395lI"
      },
      "source": [
        "Example of matching by the LOWER attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73GcQzIY5N3f",
        "outputId": "6ff1bd63-d60a-4dac-c632-8d7922acc022"
      },
      "source": [
        "doc = nlp(\"During the last decade, derivatives market became an asset class of their own and influenced the financial landscape strongly.\")\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "\n",
        "terms = [\"Asset\", \"Investment\", \"Derivatives\", \"Demand\",  \"Market\"]\n",
        "patterns = [nlp.make_doc(term) for term in terms]\n",
        "matcher.add(\"financeTerms\", None, *patterns)\n",
        "\n",
        "matches = matcher(doc)\n",
        "\n",
        "for mid, start, end in matches:\n",
        "    print(start, end, doc[start:end])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 6 derivatives\n",
            "6 7 market\n",
            "9 10 asset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPpsRlrZ-Ds3"
      },
      "source": [
        "Example of matching by the SHAPE attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4gCpprl98bI",
        "outputId": "237382d8-b024-42c4-ea01-41f2176a66fa"
      },
      "source": [
        "doc = nlp(\"This log contains the following IP addresses: 192.1.1.1 and 192.12.1.1 and 192.160.1.1 .\")\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\n",
        "\n",
        "ip_nums = [\"127.0.0.0\", \"127.256.0.0\"]\n",
        "patterns = [nlp.make_doc(ip) for ip in ip_nums]\n",
        "matcher.add(\"IPNums\", None, *patterns)\n",
        "\n",
        "for mid, start, end in matcher(doc):\n",
        "    print(start, end, doc[start:end])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 9 192.1.1.1\n",
            "12 13 192.160.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb-vr2_o-lcy"
      },
      "source": [
        "### Entity Ruler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XwDkZur-Z8n",
        "outputId": "e9a72d03-b17c-4ca4-9e0f-eac9541609d6"
      },
      "source": [
        "doc = nlp(\"Bill Gates visited Berlin.\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"ENT_TYPE\": \"PERSON\"}]\n",
        "matcher.add(\"personEnt\",  [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "\n",
        "for mid, start, end in matches:\n",
        "    print(start, end, doc[start:end])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1 Bill\n",
            "1 2 Gates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viT_v9TC--J2",
        "outputId": "4b9cc8d4-0e79-4cd0-94d9-312bc7b4dbec"
      },
      "source": [
        "doc = nlp(\"Today German chancellor Angela Merkel met with the US president.\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"ENT_TYPE\": \"PERSON\", \"OP\": \"+\"}, {\"POS\" : \"VERB\"}]\n",
        "matcher.add(\"personEntAction\",  [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "\n",
        "for mid, start, end in matches:\n",
        "    print(start, end, doc[start:end])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 6 Merkel met\n",
            "3 6 Angela Merkel met\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI8Nkylb_cyw"
      },
      "source": [
        "spaCy's EntityRuler is the component that allows us to add rules on top of the statistical model and creates an even more powerful NER model.\n",
        "\n",
        "EntityRuler is not a matcher, it's a pipeline component that we can add to our pipeline via nlp.add_pipe. When it finds a match, the match is appended to doc.ents and ent_type will be the label we pass in the pattern"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6cpzqQEAc8w",
        "outputId": "f5a12017-384d-4a1e-a761-3c67c08feeba"
      },
      "source": [
        "from spacy.pipeline import EntityRuler\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.pipe_names"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger', 'parser', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_39AmppI_IIA"
      },
      "source": [
        "doc = nlp(\"I have an acccount with chime since 2017\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5dZAEmdPOQ2"
      },
      "source": [
        "ruler = nlp.add_pipe(nlp.create_pipe('entity_ruler'))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnZRBePhRGth",
        "outputId": "627b82b9-b856-47ad-be9e-3c539abbe77f"
      },
      "source": [
        "nlp.pipe_names"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger', 'parser', 'ner', 'entity_ruler']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnUG-5AJ_zNd",
        "outputId": "dc791ff6-26a2-4fae-dd8b-ae9936b09f01"
      },
      "source": [
        "doc2 = nlp(\"I have an acccount with chime since 2017\")\n",
        "print(doc2.ents)\n",
        "print(doc2[5].ent_type_)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2017,)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R2hBUOqAyMv"
      },
      "source": [
        "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"chime\"}]}]\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "ruler.add_patterns(patterns)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}