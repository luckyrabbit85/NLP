{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40e28fc",
   "metadata": {},
   "source": [
    "# Text Cleaning and Pre-Processing\n",
    "\n",
    "Clean text is human language rearranged into a format that machine models can understand. Text cleaning can be performed using simple Python code that eliminates stopwords, removes unicode words, and simplifies complex words to their root form. In here we will see some of those techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64807cc",
   "metadata": {},
   "source": [
    "## Importing neccessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f640b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102a7e0",
   "metadata": {},
   "source": [
    "## Character Filtering\n",
    "\n",
    "We will try to filter out all the non-printable characters and extended characters.  You can find the list here: [NON PRINTABLE CHARACTERS](https://web.itu.edu.tr/sgunduz/courses/mikroisl/ascii.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1587b902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# string.printable will give the all sets of punctuation, digits, ascii_letters and whitespace.\n",
    "string.printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f612e1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~\\\\ \\\\\\t\\\\\\n\\\\\\r\\\\\\x0b\\\\\\x0c'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return string with all non-alphanumerics backslashed; this is useful if you want to match an\n",
    "# arbitrary literal string that may have regular expression metacharacters in it.\n",
    "re.escape(string.printable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07fbe601",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('[^%s]' %re.escape(string.printable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92380618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#@', '€', 'Hi', '‡', 'x', '™', 'is', '®', 'z', '<|>', '§character', '¥']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_chars = \"#@ € Hi ‡ x ™ is ® z <|> §character ¥\"\n",
    "extended_chars = extended_chars.split()\n",
    "extended_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c49894b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#@', '', 'Hi', '', 'x', '', 'is', '', 'z', '<|>', 'character', '']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printable = [pattern.sub('', word) for word in extended_chars]\n",
    "printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb8be79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#@  Hi  x  is  z <|> character '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(printable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cae9b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4533d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = \"Ratification et mise en œuvre des conventions de l'OIT mises à jour (vote)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280ba932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ratification et mise en uvre des conventions de l'OIT mises a jour (vote)\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr = normalize('NFD', fr).encode('ascii', 'ignore')\n",
    "fr = fr.decode('UTF-8')\n",
    "fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cefcdb",
   "metadata": {},
   "source": [
    "## Making use of string.maketrans() to remove punctuations\n",
    "\n",
    "Python string method maketrans() returns a translation table that maps each character in the intabstring into the character at the same position in the outtab string. Then this table is passed to the translate() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5b91c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71323fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello  Pam \n"
     ]
    }
   ],
   "source": [
    "text = \"<Hello <*^_^*> Sam!> \"\n",
    "table = str.maketrans(\"S\", \"P\", string.punctuation) # first two arguments are mapping and the third is to remove\n",
    "print(text.translate(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6be9a2",
   "metadata": {},
   "source": [
    "## Convert Unicode Characters to ASCII String in Python\n",
    "\n",
    "The Python module unicodedata provides a way to utilize the database of characters in Unicode and utility functions that help the accessing, filtering, and lookup of these characters significantly easier.\n",
    "\n",
    "unicodedata has a function called normalize() that accepts two parameters, the normalized form of the Unicode string and the given string.\n",
    "There are 4 types of normalized Unicode forms: NFC, NFKC, NFD, and NFKD. For more information [Official documentation](https://unicode.org/faq/normalization.html#4b).\n",
    "\n",
    "NFD - Normalisation Form Canonical Decomposition  \n",
    "NFC - Normalisation Form Canonical Composition  \n",
    "NFKD - Normalisation Form Compatibility Decomposition  \n",
    "NFKC - Normalisation Form Compatibility Composition  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2fde7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Har ar ett exempel pa en svensk mening att ge dig.'\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "stringVal = u'Här är ett exempel på en svensk mening att ge dig.'\n",
    "stringVal = unicodedata.normalize('NFKD', stringVal).encode('ascii', 'ignore')\n",
    "print(stringVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8d7da02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Har ar ett exempel pa en svensk mening att ge dig.\n"
     ]
    }
   ],
   "source": [
    "stringVal = stringVal.decode('UTF-8')\n",
    "print(stringVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9871a",
   "metadata": {},
   "source": [
    "## Cleaning Text\n",
    "\n",
    "**Note:**  It is not neccessary that we use every step in the function below. We should procced in a manner keeping in mind the task for which we are cleaning the text. Example: Sentiment classification, language translation, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9792d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(dataFrame):\n",
    "    \n",
    "    cleanedText = []\n",
    "    lines = dataFrame[\"Review_text\"].values.tolist()\n",
    "    \n",
    "    for text in lines:\n",
    "        # Converting text to lower case\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Removing hyperlinks\n",
    "        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text = pattern.sub(\"\", text)\n",
    "        \n",
    "        # Removing Emojis\n",
    "        emoji = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        \n",
    "        text = emoji.sub(r'', text)\n",
    "        \n",
    "        # Normalizing unicode characters\n",
    "        text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore')\n",
    "        text = text.decode('UTF-8')\n",
    "        \n",
    "        # Replacing common abbrevated words\n",
    "        text = re.sub(r\"i'm\", \"i am\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)        \n",
    "        text = re.sub(r\"what's\", \"what is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text) \n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)  \n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)  \n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"don't\", \"do not\", text)\n",
    "        text = re.sub(r\"did't\", \"did not\", text)\n",
    "        text = re.sub(r\"can't\", \"can not\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "        text = re.sub(r\"have't\", \"have not\", text)\n",
    "        \n",
    "        # Removing punctuations\n",
    "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "        \n",
    "        # Tokenizing\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Removing puntuation\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        \n",
    "        # Removing alpha numeric\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        \n",
    "        # Removing non-printable characters\n",
    "        pattern = re.compile('[^%s]' % re.escape(string.printable))\n",
    "        words = [pattern.sub('', word) for word in words]\n",
    "        \n",
    "#         # Removing Stop Words\n",
    "#         stop_words = set(stopwords.words(\"english\"))\n",
    "#         stop_words.discard(\"not\")\n",
    "#         words = [word for word in words if not word in stop_words]\n",
    "        \n",
    "#         # Stemming words\n",
    "#         stemmer = PorterStemmer()\n",
    "#         words = [stemmer.stem(word) for word in words]\n",
    "        \n",
    "#         # Lemmatization\n",
    "#         lemmatizer = WordNetLemmatizer()\n",
    "#         words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        \n",
    "        text = \" \".join(words)\n",
    "        cleanedText.append(text)\n",
    "    return cleanedText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1759130",
   "metadata": {},
   "source": [
    "## Spell Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "892bc9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "382c3e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have good spelling!\n"
     ]
    }
   ],
   "source": [
    "b = TextBlob(\"I havv goood speling!\")\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "532607f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He would have reached home by now\n"
     ]
    }
   ],
   "source": [
    "b = TextBlob(\"He woud havv reachd home by noww\")\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866627c",
   "metadata": {},
   "source": [
    "**Additional resources:**\n",
    "\n",
    "**NLTK**  \n",
    "https://www.nltk.org/index.html  \n",
    "http://www.nltk.org/book/  \n",
    "\n",
    "**Spacy**  \n",
    "https://spacy.io/usage/spacy-101  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4608a0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
